import runpod
from rembg import remove, new_session
from PIL import Image
import torch
import base64
import io
import logging
import traceback
import time
import open_clip

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)

# âœ… åŠ è½½å›¾ç‰‡æ¨¡å‹ï¼ˆä» HuggingFace Hub åŠ è½½ï¼‰ï¼Œä¸èƒ½è‡ªå®šä¹‰
image_model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(
    "hf-hub:Marqo/marqo-fashionCLIP",
    precision="fp16"
)
image_model = image_model.cuda().eval()
image_processor = preprocess_val

rembg_session = new_session("u2netp")

# æ˜¾å¼æ¸…ç©ºåˆå§‹ç¼“å­˜
torch.cuda.empty_cache()

# æ‰“å°å½“å‰GPUä¿¡æ¯
logging.info(f"ğŸš€ å½“å‰ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")

# CUDA é¢„çƒ­ - image_model
with torch.no_grad(), torch.cuda.amp.autocast():
    dummy_image = Image.new('RGB', (224, 224), color=(255, 255, 255))
    tensor_image = image_processor(dummy_image).unsqueeze(0).cuda()
    _ = image_model.encode_image(tensor_image, normalize=True)
logging.info("âœ… å›¾ç‰‡æ¨¡å‹ warmup å®Œæˆ")

# æ ¸å¿ƒå¤„ç†å‡½æ•°
def handler(job):
    # æ·»åŠ å†…å­˜æ¸…ç†
    torch.cuda.empty_cache()
    logging.info(f"ğŸ§¹ æ¸…ç†GPUç¼“å­˜ï¼Œå½“å‰å†…å­˜çŠ¶æ€: {torch.cuda.memory_summary()}")
    logging.info(f"ğŸ“¥ ä»»åŠ¡è¾“å…¥å†…å®¹:\n{job}\nğŸ“„ ç±»å‹: {type(job)}")
    try:
        inputs = job["input"].get("data")
        logging.info(f"ğŸ“‹ inputså†…å®¹æ˜¯: {inputs} (ç±»å‹: {type(inputs)}, é•¿åº¦: {len(inputs) if inputs else 0})")
        if isinstance(inputs, str):
            inputs = [inputs]

        if not inputs:
            logging.warning("âš ï¸ æ•°æ®ä¸ºç©º")
            return {
                "output": {
                    "error": "Empty input provided."
                }
            }

        start_time = time.time()

        images = []
        for i, img_str in enumerate(inputs):
            img_start_time = time.time()
            if img_str.startswith("data:image/"):
                img_str = img_str.split(",")[1]
            image = Image.open(io.BytesIO(base64.b64decode(img_str)))
            decode_time = time.time()
            logging.info(f"ğŸ–¼ï¸ è§£ç ç¬¬{i}å¼ å›¾ç‰‡è€—æ—¶: {decode_time - img_start_time:.3f}s")

            image = remove(image, session=rembg_session).convert("RGB")
            rembg_time = time.time()
            logging.info(f"ğŸ§¹ å»èƒŒæ™¯ç¬¬{i}å¼ å›¾ç‰‡è€—æ—¶: {rembg_time - decode_time:.3f}s")

            images.append(image)

        try:
            processed_images = torch.stack([image_processor(img) for img in images]).cuda()
        except Exception as e:
            logging.error(f"âŒ å›¾ç‰‡å¤„ç†å‡ºé”™: {str(e)}")
            traceback.print_exc()
            torch.cuda.empty_cache()
            return {
                "output": {
                    "error": f"Image processing error: {str(e)}",
                    "trace": traceback.format_exc()
                }
            }

        processor_time = time.time()
        logging.info(f"ğŸ›ï¸ å›¾ç‰‡æ‰¹å¤„ç†è€—æ—¶: {processor_time - rembg_time:.3f}s")

        with torch.no_grad(), torch.cuda.amp.autocast():
            vectors = image_model.encode_image(processed_images, normalize=True).cpu().tolist()
        
        # æ˜¾å¼é‡Šæ”¾å¤„ç†åçš„å›¾ç‰‡å¼ é‡
        del processed_images
        torch.cuda.empty_cache()

        inference_time = time.time()
        logging.info(f"â±ï¸ å›¾ç‰‡æ¨ç†è€—æ—¶: {inference_time - processor_time:.3f}s")

        total_time = time.time()
        logging.info(f"âœ… æ€»å›¾ç‰‡å¤„ç†æ—¶é—´: {total_time - start_time:.3f}s")
        logging.info(f"âœ… æ¨ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(vectors)} ä¸ªembeddingï¼Œæ¯ä¸ªembeddingç»´åº¦: {len(vectors[0])}")
        logging.info(f"ğŸ’¾ æœ€ç»ˆå†…å­˜çŠ¶æ€: {torch.cuda.memory_summary()}")

        return {
            "output": {
                "embeddings": vectors
            }
        }

    except Exception as e:
        logging.error(f"âŒ å‡ºç°å¼‚å¸¸: {str(e)}")
        traceback.print_exc()
        torch.cuda.empty_cache()
        return {
            "output": {
                "error": str(e),
                "trace": traceback.format_exc()
            }
        }

# å¯åŠ¨ Serverless Worker
logging.info("ğŸŸ¢ Image Worker å·²å¯åŠ¨ï¼Œç­‰å¾…ä»»åŠ¡ä¸­...")
runpod.serverless.start({"handler": handler})